{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "models that are available through hugging face & the transformers packages\n",
    "Neural Network models\n",
    "take inputs and transforms into  numerical representations\n",
    "\n",
    "Core components\n",
    "- Encoders\n",
    "- Decoders\n",
    "- self-attention mechanism\n",
    "\n",
    "Usecases for text, image, vision, and audio\n",
    "Classification available across all above\n",
    "\n",
    "Data can be used to train multiple models. So if I wanna train a similiar new task, I don't need to input a large amount of new data. (Also why very very specific tasks cant be completed with Hugging face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use Huggingface hub\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "#view available models\n",
    "\n",
    "# list(api.list_models())\n",
    "\n",
    "#search for specific models\n",
    "models = api.list_models(\n",
    "    filter=ModelFilter(\n",
    "        task='text-classification',\n",
    "        sort='downloads',\n",
    "        order='desc',\n",
    "        limit=5\n",
    "    )\n",
    ")\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving a model locally\n",
    "from transformers import AutoModel\n",
    "modelId = 'bert-base-uncased'\n",
    "#download\n",
    "model = AutoModel.from_pretrained(modelId)\n",
    "#save to local directory\n",
    "model.save_pretrained(save_directory=f'models/{modelId}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets in HF uses Apache Arrow structure. Column based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\judyz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\judyz\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder, load_dataset\n",
    "\n",
    "data_builder = load_dataset_builder('imdb')\n",
    "print(data_builder.info.description)  # info\n",
    "print(data_builder.info.features)  # list of columns\n",
    "\n",
    "data = load_dataset('imdb')  # to load dataset\n",
    "\n",
    "# To mutate the dataset\n",
    "imdb=load_dataset('imdb',split='train')\n",
    "#Filter\n",
    "filtered = imdb.filter(lambda row: row['label']==0)  # filter to row with label == 0\n",
    "# Sclising\n",
    "sliced = filtered.select(range(2))  # select rows based on indices; selected rows 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines with Hugging Face\n",
    "When you click \"use in transformers\" when looking at a model on web...\n",
    "\n",
    "\n",
    "Auto Class - general classes for using models and tokenizers, config, processors, feature extraction... flexible and direct.\n",
    "- use to directly download model\n",
    "Use Cases:\n",
    "- Fine-tuning a pre-trained model on a specific task (e.g., classification, translation).\n",
    "- Building custom workflows by combining a model, tokenizer, and other components.\n",
    "- Advanced tasks requiring direct control over the model architecture and inputs/outputs.\n",
    "\n",
    "Pipeline - more hands on; great for getting started\n",
    "A pipeline simplifies the process of loading a pre-trained model and performing inference. It bundles the model, tokenizer, and preprocessing/postprocessing into a single interface.\n",
    "- task-specific pipelines for each task\n",
    "- leverage auto classes behind the scene\n",
    "Use Cases:\n",
    "- Quickly prototyping or running inference without needing detailed knowledge of the model’s internals.\n",
    "- Non-developers or beginners who need easy access to NLP models for common tasks.\n",
    "- Tasks where you don’t require much customization of the underlying model.\n",
    "\n",
    "Having chatgpt explaining the differences: https://chatgpt.com/share/677cc85e-2004-8013-8101-ed7cc3b3f58e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "# creating pipeline\n",
    "my_pipeline = pipeline(task='text-classification', model='...')  # if not specifying model, it will use the default model\n",
    "input = \"text classification sample text\"\n",
    "my_pipeline(input)\n",
    "\n",
    "\n",
    "# With auto class\n",
    "# Download the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Create the pipeline\n",
    "sentimentAnalysis = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Predict the sentiment\n",
    "output = sentimentAnalysis(input)\n",
    "\n",
    "print(f\"Sentiment using AutoClasses: {output[0]['label']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "- convert components of natural language into tokens (numerical)\n",
    "\n",
    "#### Normalization\n",
    "- cleaning text & removing white spaces\n",
    "- accents removed \n",
    "- lowercasing\n",
    "\n",
    "#### pre-tokenization\n",
    "- split text into small tokens\n",
    "\n",
    "Different tokenization models have different tokenizing methods\n",
    "GOAL: to create a library of vocabularies and understand the patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use noarmlizer\n",
    "# Import the AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# Download the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Normalize the input string\n",
    "output = tokenizer.backend_tokenizer.normalizer.normalize_str(input_string)\n",
    "\n",
    "\n",
    "# Use tokenizer\n",
    "# CONFUSED: why not calling AutoTokenizer?\n",
    "# AutoTokenizer is a flexible and generic class that automatically selects the correct tokenizer for a model based on its identifier or configuration.\n",
    "# When you specify \"gpt2\" as the model name, AutoTokenizer internally resolves it to GPT2Tokenizer.\n",
    "\n",
    "# Download the gpt tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Tokenize the input\n",
    "gpt_tokens = gpt_tokenizer.tokenize(input)\n",
    "# Repeat for distilbert\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "distil_tokens = distil_tokenizer.tokenize(text=input)\n",
    "# Compare the output\n",
    "print(f\"GPT tokenizer: {gpt_tokens}\")\n",
    "print(f\"DistilBERT tokenizer: {distil_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text classification\n",
    "- sentiment analysis\n",
    "- grammatical correctness\n",
    "- judging if an answer is logically correct for a question (entailment. Question Natural Language Inference, or QNLI)\n",
    "\n",
    "zeroshot training model: best when with limited resources, no need for specific training\n",
    "Zero-shot classification is the ability for a transformer to predict a label from a new set of classes which it wasn't originally trained to identify.\n",
    "only requires inputs (text) and labels\n",
    "\n",
    "\n",
    "#### Summarization\n",
    "- representative information is extracted using sentence scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'sshlefier/distilbar-cnn-12-6'\n",
    "summarizer = pipeline(task='summarization', model=model)\n",
    "text=\"xsxsssgqwegwq\"\n",
    "summary_text=summarizer(text)\n",
    "print(summary_text[0]['summary_text'])\n",
    "\n",
    "# Each model has specific set of parameters used for adjusting the model \n",
    "# min_length & Max_length\n",
    "# max_lenghth defaulted to 142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForCausalLM # frequently used for text generation\n",
    "\n",
    "# Set model name\n",
    "model_name = \"gpt2\"\n",
    "# Get the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Wear sunglasses when its sunny because\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")  # PyTorch tensors are similar to arrays. It's more compatible with gpt2\n",
    "output = model.generate(input_ids, num_return_sequences=1)  # return only 1 word\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation from images\n",
    "## Generating caption for an image\n",
    "\n",
    "# Get the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
    "# Process the image\n",
    "pixels = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "# Generate the ids\n",
    "output = model.generate(pixel_values=pixels)\n",
    "# Decode the output\n",
    "caption = processor.batch_decode(output)\n",
    "print(caption[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
